## References

1. Birhane, A., Kalluri, P., Card, D., Agnew, W., Dotan, R., and Bao, M. (2022). The Values Encoded in Machine Learning Research. In *FAccT '22*. arXiv:2106.15590.
2. Casper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer, J., Rando, J., ... (2023). Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback. *TMLR*. arXiv:2307.15217.
3. Dubois, Y., Galambosi, B., Liang, P., and Hashimoto, T. B. (2024). Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators. arXiv:2404.04475.
4. Ethayarajh, K. and Jurafsky, D. (2020). Utility is in the Eye of the User: A Critique of NLP Leaderboards. In *EMNLP 2020*. arXiv:2009.13888.
5. Ganguli, D., et al. (2022). Predictability and Surprise in Large Generative Models. In *FAccT '22*. arXiv:2202.07785.
6. Ganguli, D., et al. (2022). Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned. arXiv:2209.07858.
7. Juzek, T. S. and Ward, Z. B. (2025). Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback. arXiv:2508.01930.
8. Kiela, D., Bartolo, M., et al. (2021). Dynabench: Rethinking Benchmarking in NLP. In *NAACL 2021*. arXiv:2104.14337.
9. Kirk, H. R., Mediratta, I., Nalmpantis, C., Luketina, J., Hambro, E., Grefenstette, E., and Raileanu, R. (2023). Understanding the Effects of RLHF on LLM Generalisation and Diversity. arXiv:2310.06452.
10. Kirk, H. R., Vidgen, B., Rottger, P., et al. (2024). The Benefits, Risks and Bounds of Personalizing the Alignment of Large Language Models to Individuals. *Nature Machine Intelligence*, 6, 383-392.
11. Murthy, S. K., et al. (2024). One Fish, Two Fish, but Not the Whole Sea: Alignment Reduces Language Models' Conceptual Diversity. In *NAACL 2025*. arXiv:2411.04427.
12. Perez, E., et al. (2023). Discovering Language Model Behaviors with Model-Written Evaluations. In *ACL 2023 Findings*. arXiv:2212.09251.
13. Ren, R., et al. (2025). The MASK Benchmark: Disentangling Honesty From Accuracy in AI Systems. arXiv:2503.03750.
14. Rottger, P., et al. (2024). XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models. In *NAACL 2024*. arXiv:2308.01263.
15. Santurkar, S., Durmus, E., Ladhak, F., Lee, C., Liang, P., and Hashimoto, T. (2023). Whose Opinions Do Language Models Reflect? In *ICML 2023*. arXiv:2303.17548.
16. Sharma, M., Tong, M., Korbak, T., Duvenaud, D., Askell, A., Bowman, S. R., et al. (2024). Towards Understanding Sycophancy in Language Models. In *ICLR 2024*. arXiv:2310.13548.
17. Sourati, Z., Ziabari, A. S., and Dehghani, M. (2025). The Homogenizing Effect of Large Language Models on Human Expression and Thought. arXiv:2508.01491.
18. Xu, R., et al. (2024). On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization. *JASA*. arXiv:2405.16455.
19. Zheng, L., Chiang, W.-L., et al. (2023). Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. In *NeurIPS 2023 Datasets and Benchmarks*. arXiv:2306.05685.
20. Bhatia, A., et al. (2025). Value Drifts: Tracing Value Alignment During LLM Post-Training. arXiv:2510.26707.
21. Rath, A. (2026). Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems. arXiv:2601.04170.
22. Muthukumar, K. (2025). Empathy AI in Healthcare. *Frontiers in Psychology*, 16. doi:10.3389/fpsyg.2025.1680552.
23. Benjamini, Y. and Hochberg, Y. (1995). Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing. *Journal of the Royal Statistical Society: Series B*, 57(1), 289-300.
24. McCarthy, P. M. and Jarvis, S. (2010). MTLD, vocd-D, and HD-D: A Validation Study of Sophisticated Approaches to Lexical Diversity Assessment. *Behavior Research Methods*, 42(2), 381-392.
25. Heiner, N. and Wood, K. (2026). Bringing Light to the GPT-4o vs. GPT-5 Personality Controversy. *SurgeHQ Blog*. <https://surgehq.ai/blog/bringing-light-to-the-gpt-4o-vs-gpt-5-personality-controversy>.
26. Serapio-García, G., Safdari, M., Crepy, C., Sun, L., Fitz, S., Romero, P., Abdulhai, M., Faust, A., and Matarić, M. (2025). A Psychometric Framework for Evaluating and Shaping Personality Traits in Large Language Models. *Nature Machine Intelligence*. doi:10.1038/s42256-025-01115-6.
27. Chen, R., Arditi, A., Sleight, H., Evans, O., and Lindsey, J. (2025). Persona Vectors: Monitoring and Controlling Character Traits in Language Models. Anthropic. arXiv:2507.21509.
28. Altman, S. (2025). "We missed the mark with last week's GPT-4o update." *X/Twitter*, May 2, 2025. <https://x.com/sama/status/1918330652325458387>.
