### C. Related Work

Our study draws on and contributes to five intersecting research areas: RLHF limitations, LLM-as-judge methodology, benchmark measurement gaps, false refusal and sycophancy, and alignment effects on expressive diversity.

**RLHF Limitations.** Casper et al. (arXiv:2307.15217) provide the definitive survey of RLHF's open problems, including reward model misspecification and the fundamental inability of human evaluators to detect subtle model failures. Xu et al. (arXiv:2405.16455) formalize one such failure as *preference collapse*: standard RLHF's KL-regularization causes majority preferences to dominate, with minority viewpoints receiving near-zero probability mass. Santurkar et al. (arXiv:2303.17548) demonstrate the downstream consequence -- substantial misalignment between LM opinions and those of diverse demographic groups, persisting even after explicit steering. Our alignment tax concept operationalizes the cumulative effect of these documented mechanisms: each round of alignment optimization narrows the model's behavioral repertoire along dimensions that reward models do not capture.

**LLM-as-Judge Methodology.** Our evaluation employs blind LLM-as-judge scoring, an approach validated by Zheng et al. (arXiv:2306.05685), who demonstrate >80% agreement between strong LLM judges and human preferences. However, Zheng et al. also document systematic biases -- position, verbosity, and self-enhancement -- that motivate our inter-rater reliability checks (Fleiss' kappa = 0.765). Dubois et al. (arXiv:2404.04475) show that automatic evaluators systematically favor longer responses, with length-controlled evaluation raising Chatbot Arena correlation from 0.94 to 0.98. This finding is directly relevant: gpt-5.2-chat produces substantially longer responses than chatgpt-4o-latest, meaning that uncorrected LLM-as-judge evaluation would systematically overstate 5.2-chat's quality. Our scoring rubric explicitly penalizes unnecessary verbosity and evaluates quality independently of length.

**Benchmark Measurement Gaps.** The divergence between benchmark performance and user-perceived quality has deep roots. Kiela et al. (arXiv:2104.14337) demonstrate that static benchmarks saturate rapidly while models fail on simple real-world challenges, proposing dynamic evaluation as an alternative. Ethayarajh and Jurafsky (arXiv:2009.13888) formalize this through microeconomic theory: leaderboard metrics systematically exclude costs borne by practitioners (latency, efficiency), creating evaluation blindspots. Birhane et al. (arXiv:2106.15590) find that the ML research community itself rarely considers negative consequences, helping explain why alignment metrics track narrow performance rather than communicative quality. Our Benchmark Bridge suite operationalizes this gap by embedding both benchmark-verifiable tasks and human-quality dimensions within the same questions, enabling direct comparison of the dimensions on which models converge versus diverge.

**False Refusal and Sycophancy.** Rottger et al. (arXiv:2308.01263) introduce XSTest, the first systematic false refusal benchmark, documenting exaggerated safety behaviors where models refuse clearly safe prompts sharing surface features with unsafe ones. Our FRR battery extends this approach with absurd-context questions designed to isolate keyword-level from semantic-level safety reasoning. The gradient we observe -- 4.0% (4o) to 7.3% (5.1) to 17.7% (5.2), χ²=20.5, p<10⁻⁴ -- quantifies what Rottger et al. identified qualitatively. On the sycophancy axis, Perez et al. (arXiv:2212.09251) demonstrate that larger models become more sycophantic (>90% for 52B parameters) and that RLHF does not train it away. Sharma et al. (arXiv:2310.13548) extend this finding at ICLR 2024, showing that both humans and preference models prefer sycophantic responses over correct ones a non-negligible fraction of the time -- establishing the feedback loop that our cross-generational data traces through three model versions.

**Alignment Effects on Expressive Diversity.** Perhaps most directly relevant to our lexical findings, Kirk et al. (arXiv:2310.06452) demonstrate that RLHF significantly reduces output diversity compared to SFT, establishing a documented tradeoff between generalization and diversity. Murthy et al. (arXiv:2411.04427) confirm this with a distinct methodology: aligned models display less conceptual diversity than instruction-fine-tuned counterparts across word-color associations and similarity judgments. Juzek and Ward (arXiv:2508.01930) identify the mechanism: human raters systematically prefer certain words, creating a feedback loop through alignment training that narrows vocabulary. Sourati et al. (arXiv:2508.01491) synthesize evidence across disciplines showing that LLM-driven homogenization extends beyond vocabulary to cognitive diversity itself. Our TTR decline (0.563 to 0.545 across two generations) and exclamation extinction (33x reduction) are specific instances of this broader homogenization pattern, now measured in a controlled cross-generational comparison rather than a single model snapshot.

**Existing Evaluation Frameworks.** The MASK benchmark (arXiv:2503.03750) demonstrates that scaling improves factual accuracy but *worsens* honesty -- no model exceeds 46% honest accuracy under social pressure. Our false refusal data replicates this anti-scaling pattern in the safety domain. The CCQ framework (Frontiers in Psychology, 2025) provides a structured empathy assessment protocol; our Sycophancy-Empathy suite adapts this approach to distinguish genuine empathy from sycophantic agreement through triangulation of factual accuracy and emotional attunement. Ganguli et al. (arXiv:2202.07785) argue that generative models combine predictable scaling-law improvements with unpredictable emergent behaviors; our alignment tax is an instance of this pattern, where benchmark performance improves predictably while communicative quality degrades in ways invisible to the metrics guiding development.

Together, these works establish the theoretical and empirical groundwork for our central claim: that alignment optimization imposes a measurable cost on dimensions that current evaluation frameworks do not track. Our contribution is to provide the first controlled, cross-generational measurement of this cost using the same questions, the same rubric, and the same judge across three models in direct succession.
